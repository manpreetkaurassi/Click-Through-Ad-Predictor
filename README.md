# Click-Through-Ad-Predictor

The data was collected from - https://www.kaggle.com/c/avazu-ctr-prediction/data

We used the train data from the Kaggle competition to predict the probability, that given some key information about the ad presented to a user on the phone, he will click on the Ad. Since Ads are major revenue generators of major Internet companies, being able to predict the probability of click is an important study. Revenue is the product of the bid and probability of the ad getting clicked. Out of the two variables, bid is what the company decides, leaving us with controlling the revenue based on the ability to predict the click. This is a machine learning problem is a classification problem with probabilistic outputs. 

Our machine learning objective which determined how good our model was log- loss. We log-loss instead of accuracy or Area under the Curve is because log-loss penalizes, misclassifications. Also, since revenue is the multiplication of bid and probability of click, log-loss was finalized as the best approach. Where, AUC could have been used, if the problem statement was to determine the order of the ad placement. 

Positional bias is which the position the Ad is placed could impact the model, is what we had assumed. So, we decided to add weights which were inversely proportional to the historical CTR. However, this only reduced the performance of the model, although we had assumed it should have improved. 

We focused on finding a model which would fit in the machine learning constraints of an Ad click type problem, which are, low latency, relevance, interpretability, parallelizability.  We understood various models before implementing logistic regression and gradient boosting model. Naive Bayes model, although simple to run, low latency and high interpretability, assumes that the features are gives probability value which are vague. KNN model has a very high run time complexity and hence, ignored it. Linear support-vector machines models are not optimizing explicitly for log-loss and Kernel SVM were very hard to train. We also considered stacking model, but since low latency was one of our criteria, we decided to not go ahead with that. Logistic Regression was a major choice since it has low latency and most importantly, the model internally tries to minimize the log-loss value, However, the drawback was the it was highly likely that the data is not linearly separable, and such was the case with our dataset. We went ahead and implemented the Gradient Boosting algorithm, which after training gave us a log loss value of 0. 40.
